{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Straight Line to (Fake) Data\n",
    "\n",
    "**AY128/256 UC Berkeley**\n",
    "\n",
    "## A simple, practical example of\n",
    "* Bayesian Inference \n",
    "* Markov chain Monte Carlo (MCMC)\n",
    "\n",
    "For this example, you'll need to have `emcee` and `corner` installed.\n",
    "\n",
    "```bash\n",
    "pip install corner emcee\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Fake Dataset\n",
    "\n",
    "* N data points \n",
    "* Linear model : $y = mx + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ndata = 10 # number of data points\n",
    "m_true = 3 # true slope\n",
    "b_true = -2 # true intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Gaussian Noise to the fake data\n",
    "\n",
    "* Compute our model: $y = mx + b$\n",
    "* Compute error bars, by randomly drawing them from a uniform distribution [0.1,0.7]\n",
    "* Assume that each data point $y$ is the mean of a normal distribution and that error bar is the standard deviation\n",
    "* Then we can resample the model to make it look noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "seed =   128  \n",
    "rnd = np.random.RandomState(seed)\n",
    "\n",
    "# create a set of x points with length N data from a uniform distritubion\n",
    "xmin, xmax = 0, 2\n",
    "x = rnd.uniform(xmin, xmax, Ndata)\n",
    "x.sort()  # sort them in place\n",
    "\n",
    "# create noiseless fake y data using x and true slope and intercept\n",
    "y_data = m_true * x + b_true\n",
    "\n",
    "# create y error bars from a uniform distribution\n",
    "y_err_min, y_err_max = 0.1, 0.7\n",
    "y_err = rnd.uniform(y_err_min, y_err_max, Ndata)\n",
    "\n",
    "# add Gaussian noise to fake y data by re-sampling\n",
    "# assume each noiseless y data point (from above) and y_err are the mean\n",
    "# and standard deviation of a normal distribution\n",
    "# use this to make noisy data\n",
    "y_data = rnd.normal(y_data, y_err)\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(x, y_data, yerr=y_err, ls='none', marker='o', ms=10, c='orange')\n",
    "plt.xlabel(\"$x$\", fontsize=20)\n",
    "plt.ylabel(\"$y$\", fontsize=20)\n",
    "plt.xlim(-0.05, 2)\n",
    "plt.ylim(-3, 5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretend we don't the know how the fake data were generated...\n",
    "\n",
    "* Looks like a linear model is reasonable: $y = mx + b$\n",
    "* Two parameters: $m$, $b$\n",
    "* Assume errors are normally distributed\n",
    "* Assume data are drawn independently and identically (IID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write down model-data relationship using Bayesian notation\n",
    "\n",
    "## In linear space:\n",
    "\n",
    "## $P(m,b \\mid D) \\propto\\ P(D \\mid m,b) \\ P(m,b)$\n",
    "\n",
    "## In log space:\n",
    "\n",
    "## $\\log{P(m,b \\mid D)} \\propto\\ \\log{P(D \\mid m,b)} + \\log{P(m,b)}$\n",
    "* Note that log = natural log (not base 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define the likelihood function for single data point\n",
    "* Likelihood function (assuming IID and normally distributed data/errors):\n",
    "   ## $P(D_i \\mid m,b) = \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}} e^{-\\frac{(D_i - y_i)^2}{2\\sigma_i^2}}$\n",
    "* log-Likelihood function:\n",
    "   ##  $\\log{P(D_i \\mid m,b)} \\propto\\ -0.5 \\big( {{\\frac{(D_i - y_i)^2}{\\sigma_i^2}}} - \\log{(\\frac{1}{\\sigma_i^2})} \\big)$\n",
    "   \n",
    "   * Some constants got dropped (e.g., $\\pi$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, assuming IID, let's write down the likelihood function for the entire dataset:\n",
    "* ### Linear space: IID $\\rightarrow P(D \\mid m,b) = \\prod\\limits_{i} P(D_i \\mid m,b)$\n",
    "* ### log space: IID $\\rightarrow \\log{P(D \\mid m,b)} = \\sum\\limits_{i} \\log{P(D_i \\mid m,b)}$\n",
    "* ### log space: $\\log{P(D \\mid m,b)} = -0.5 \\ \\sum\\limits_{i}   \\big({{\\frac{(D_i - y_i)^2}{\\sigma_i^2}}} - \\log{(\\frac{1}{\\sigma_i^2})} \\big)$\n",
    "\n",
    "### The log-likelihood function in python with $\\theta = (m,b)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for log-likelihood\n",
    "# assumes theta = {m,b}\n",
    "# takes theta and data: (x,y,yerr)\n",
    "# returns log-likelihood value for entire dataset \n",
    "# given one value of m and one value of b\n",
    "def lnlike(theta, x, y, yerr):\n",
    "    m, b = theta\n",
    "    model = m * x + b\n",
    "    inv_sigma2 = 1.0/(yerr**2)\n",
    "    return -0.5*(np.sum((y-model)**2*inv_sigma2 - np.log(inv_sigma2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide on some priors:\n",
    "\n",
    "* Using simple top-hat priors\n",
    " - Simple to implement, but they qualitfy as \"uninformative\"\n",
    " \n",
    "\n",
    "*   $P(m,b) = \\left\\{\\begin{array}{lr}\n",
    "        1, & \\text{for } -5\\leq m\\leq 5 \\ \\text{and} -5\\leq b\\leq 5 \\\\\n",
    "        0, & \\text{otherwise }\n",
    "        \\end{array}\\right\\}$\n",
    "        \n",
    " \n",
    "*   $\\log{P(m,b)} = \\left\\{\\begin{array}{lr}\n",
    "        0, & \\text{for } -5\\leq m\\leq 5 \\ \\text{and} -5\\leq b\\leq 5 \\\\\n",
    "        -\\infty, & \\text{otherwise }\n",
    "        \\end{array}\\right\\}$\n",
    "\n",
    "### The log-prior in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that takes in theta - (m,b) and \n",
    "# returns log-prior if priors are within bounds \n",
    "# return -inf if priors are outside of the bounds\n",
    "# if you're sampler can't handle -inf, you can use a very small number\n",
    "# (e.g., -1e6) as a kludge\n",
    "def lnprior(theta):\n",
    "    m, b = theta\n",
    "    if -5.0 <= m <= 5.0 and -5 <= b <= 5:\n",
    "        return 0.0\n",
    "    return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior:\n",
    "\n",
    "The log-posterior is the sum of the log-likelihood and log-prior: \n",
    "## $\\log{P(m,b \\mid D)} = \\log{P(D \\mid m,b)} + \\log{P(m,b)}$.\n",
    "\n",
    "### The log-posterior function in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes in theta=(m,b) and data = (x,y, yerr)\n",
    "# returns a log probability if finite\n",
    "# retrns -inf if not finite\n",
    "def lnprob(theta, x, y, yerr):\n",
    "    lp = lnprior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(theta, x, y, yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the posterior using emcee:\n",
    "\n",
    "* need to define: \n",
    "    - number of \"walkers\", number of dimensions (free parameters, variables, etc)\n",
    "* give walkers initial guess, which I will call $\\texttt{pos}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seed =   128  \n",
    "#rnd = np.random.RandomState(seed)\n",
    "\n",
    "ndim, nwalkers = 2, 16\n",
    "initial_m, initial_b = m_true, b_true-1\n",
    "pos = [(initial_m, initial_b) + 1e-4*rnd.randn(ndim) for i in range(nwalkers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the starting positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(pos).shape)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\texttt{pos}$ is a set of 16 pairs of initial guesses for m and b for each walker.\n",
    "\n",
    "### Import emcee and set up to run according to documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "print(f\"emcee version = {emcee.__version__}\")\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(x, y_data, y_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide on number of MCMC steps, start the sampler, and time it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "niterations = 1000\n",
    "# make sure to reset the sampler so it doesn't keep appending onto old results\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(pos, niterations)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(f\"This run took: {np.around(end_time,2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some things the sampler returns:\n",
    "\n",
    "* an object $\\texttt{sampler.chain}$ that contains parameters sampled by each walker\n",
    "* an object $\\texttt{sampler.flatchain}$ that flattens the results from each walkers as if were as single MCMC chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampler.chain.shape) # ndim, nwalker, # of parameters\n",
    "print(sampler.flatchain.shape) # ndim * nwalker, # of parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some visual diagnostics of MCMC chains\n",
    "\n",
    "*  plot values of m and b vs step number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(5,10))\n",
    "y_labels = ['$m$', '$b$']\n",
    "x_labels = 'step'\n",
    "ax[0].set_title('Parameter Value vs. Step Number for each walker')\n",
    "for i,j in enumerate(sampler.chain[0,0,:]):\n",
    "    ax[i].plot(sampler.chain[:,:,i].T, alpha=0.8, lw=0.5)\n",
    "    ax[i].set_ylabel(y_labels[i])\n",
    "    ax[i].set_xlabel(x_labels)\n",
    "plt.subplots_adjust(hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* plot values of lnP vs. step number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampler.lnprobability.T)\n",
    "plt.ylabel('lnP')\n",
    "plt.xlabel('step')\n",
    "plt.title('lnP vs. step number')\n",
    "plt.ylim(sampler.lnprobability.min(), sampler.lnprobability.max()*1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement \"burn in\" \n",
    "\n",
    "* Let's remove the first 200 steps from the chain\n",
    "\n",
    "* This is a total kludge, there are better ways to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's remove the first 200 steps as \"burn in\"\n",
    "# they are still sensitive to the initial conditions\n",
    "burn_in = 200\n",
    "samples = sampler.chain[:, burn_in:, :].reshape((-1, ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a \"corner plot\"\n",
    "\n",
    "* ### This plots \n",
    " * the marginalized 1-d probability distributions\n",
    " * the 2-d joint distribution(s)\n",
    " * sometimes called \"triangle\" plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "print(f\"corner version={corner.__version__}\")\n",
    "fig = corner.corner(samples, labels=[\"$m$\", \"$b$\"],\n",
    "                      truths=[m_true, b_true], quantiles=[0.16, 0.84])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* blue line indicates the true values of $m$ and $b$\n",
    "* 2d histrogram is the 'joint distrbiution' of $m$ and $b$\n",
    "* 1d histrograms are the 'marginalized distributions' of $m$, $b$\n",
    "* I've set the dashed lines to indicate the 16th and 84th percentiles of the distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot draws from the posterior along with data and truth\n",
    "\n",
    "* Randomly draw a model from your posterior function (e.g., pick an $m$ and $b$, and compute $y=mx+b$)\n",
    "\n",
    "* plot the line in black with low transparency\n",
    "\n",
    "* repeat 100 times (in this case)\n",
    "\n",
    "* regions of higher density illustrate regions of higher probability\n",
    "\n",
    "* Also known as a \"posterior predictive check\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of x values\n",
    "xl = np.array([0, 2])\n",
    "\n",
    "# randomly draw 100 points from my posterior (i.e., the MCMC chain)\n",
    "# and plot them as thin black lines\n",
    "for m, b in samples[np.random.randint(len(samples), size=100)]:\n",
    "    plt.plot(xl, m*xl+b, color=\"k\", alpha=0.1)\n",
    "\n",
    "# plot the true model from the very beginning\n",
    "plt.plot(xl, m_true*xl+b_true, color=\"c\", lw=2, alpha=0.8, label='Truth')\n",
    "\n",
    "# plot the fake data with error bars\n",
    "plt.errorbar(x, y_data, yerr=y_err, ls='none', marker='o', ms=10, c='orange', label='Data')\n",
    "\n",
    "#set some limits on the plot\n",
    "plt.xlabel(\"$x$\", fontsize=20)\n",
    "plt.ylabel(\"$y$\", fontsize=20)\n",
    "plt.xlim(-0.05, 2)\n",
    "plt.ylim(-3,5)\n",
    "plt.legend(frameon=False, fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's compute some summary statistics\n",
    "\n",
    "* For example, you may want to present numbers in an abstract, or list them in a table\n",
    "* A growing trend is to publish your likelihood function and/or MCMC chains\n",
    "* What point statistics to publish (e.g., maximum likelihood, maximum a posteriori, ...)?\n",
    "* Here, I've selected to compute the median, and the 16th and 84th percentiles of the 1-d PDFs, \n",
    " - This interval spans the 68% range around the median (i.e., a semi-equivalent to 1-$\\sigma$ for a normal distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute summary statistics: 50, 16, 84 percentiles\n",
    "\n",
    "m_mcmc, b_mcmc = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]),\n",
    "                             zip(*np.percentile(samples, [16, 50, 84],\n",
    "                                                axis=0)))\n",
    "print(f\"true m value: {m_true}\")\n",
    "print(f\"mcmc median, +/- for m: {m_mcmc}\\n\")\n",
    "\n",
    "print(f\"true b value: {b_true}\")\n",
    "print(f\"mcmc median, +/- for b: {b_mcmc}\")\n",
    "# note to self: figure out correct way to format f-strings + latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What should we make of this point estimate?\n",
    "\n",
    "* Is it a problem that the median values aren't the same as the true values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generally at this stage, there are lots of questions:\n",
    "* #### how do my results change with:\n",
    "    - number of data points\n",
    "    - size of error bars\n",
    "    - number of walkers\n",
    "    - number of steps\n",
    "    - initial walker position guess\n",
    "    - number of burn in steps\n",
    "    \n",
    "* #### how many walkers should I use?\n",
    "\n",
    "* #### how do I make an initial guess for walker positions?\n",
    "\n",
    "* #### how long should the chain run? or how do I know the chain is converged?\n",
    "\n",
    "* #### ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
