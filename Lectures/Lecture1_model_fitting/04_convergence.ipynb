{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".rendered_html\n",
       "{\n",
       "  color: #2C5494;\n",
       "  font-family: Ubuntu;\n",
       "  font-size: 140%;\n",
       "  line-height: 1.1;\n",
       "  margin: 0.5em 0;\n",
       "  }\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 250%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".slide-header, p.slide-header\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 200%;\n",
       "  font-weight:bold;\n",
       "  margin: 0px 20px 10px;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".rendered_html h2\n",
       "{ \n",
       "  color: #386BBC;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html h3\n",
       "{ \n",
       "  font-size: 100%;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html li\n",
       "{\n",
       "  line-height: 1.8;\n",
       "  }\n",
       "\n",
       ".input_prompt, .CodeMirror-lines, .output_area\n",
       "{\n",
       "  font-family: Consolas;\n",
       "  font-size: 120%;\n",
       "  }\n",
       "\n",
       ".gap-above\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap01\n",
       "{\n",
       "  padding-top: 10px;\n",
       "  }\n",
       "\n",
       ".gap05\n",
       "{\n",
       "  padding-top: 50px;\n",
       "  }\n",
       "\n",
       ".gap1\n",
       "{\n",
       "  padding-top: 100px;\n",
       "  }\n",
       "\n",
       ".gap2\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap3\n",
       "{\n",
       "  padding-top: 300px;\n",
       "  }\n",
       "\n",
       ".emph\n",
       "{\n",
       "  color: #386BBC;\n",
       "  }\n",
       "\n",
       ".warn\n",
       "{\n",
       "  color: red;\n",
       "  }\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AY 128/256 (UC Berkeley, 2019)\n",
    "\n",
    "Set up the problem and sample as in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "%matplotlib inline\n",
    "\n",
    "def lnlike(theta, x, y, yerr):\n",
    "    m, b = theta\n",
    "    model = m * x + b\n",
    "    inv_sigma2 = 1.0/(yerr**2)\n",
    "    return -0.5*(np.sum((y-model)**2*inv_sigma2 - np.log(inv_sigma2)))\n",
    "\n",
    "def lnprior(theta):\n",
    "    m, b = theta\n",
    "    if -5.0 <= m <= 5.0 and -5 <= b <= 5:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def lnprob(theta, x, y, yerr):\n",
    "    lp = lnprior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(theta, x, y, yerr)\n",
    "\n",
    "Ndata = 10 # number of data points\n",
    "m_true = 3 # true slope\n",
    "b_true = -2 # true intercept\n",
    "seed =   128  \n",
    "rnd = np.random.RandomState(seed)\n",
    "\n",
    "xmin, xmax = 0, 2\n",
    "x = rnd.uniform(xmin, xmax, Ndata)\n",
    "x.sort() \n",
    "y_data = m_true * x + b_true\n",
    "y_err_min, y_err_max = 0.1, 0.7\n",
    "y_err = rnd.uniform(y_err_min, y_err_max, Ndata)\n",
    "y_data = rnd.normal(y_data, y_err)\n",
    "\n",
    "ndim, nwalkers = 2, 16\n",
    "initial_m, initial_b = m_true, b_true-1\n",
    "pos = [(initial_m, initial_b) + 1e-4*rnd.randn(ndim) for i in range(nwalkers)]\n",
    "\n",
    "import emcee\n",
    "print(f\"emcee version = {emcee.__version__}\")\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(x, y_data, y_err))\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "niterations = 1000\n",
    "# make sure to reset the sampler so it doesn't keep appending onto old results\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(pos, niterations)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(f\"This run took: {np.around(end_time,2)} seconds\")\n",
    "print(sampler.chain.shape) # ndim, nwalker, # of parameters\n",
    "print(sampler.flatchain.shape) # ndim * nwalker, # of parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have samples from the posterior we can do cool things:\n",
    "\n",
    "Suppose we have B samples $\\theta_1$...$\\theta_B$ from the posterior $p(\\theta|$**X**):\n",
    "\n",
    "1) **Posterior mean**: \n",
    "   \n",
    "The exact equation $E[\\theta|$**X**] = $\\int \\theta p(\\theta|$**X**)$d\\theta$\n",
    "\n",
    "Using the sample $E[\\theta|$**X**] $\\approx \\frac{1}{B} \\sum_{b=1}^B \\theta_b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean m from the MCMC chain = {sampler.chain[:,200::3,0].sum()/sampler.chain[:,200::3,0].size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the ::3 above....this is me prunning the chains to every third entry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Marginalization**: \n",
    "   \n",
    "The exact equation $p(\\theta_1|$**X**) = $\\int p(\\theta_1,\\theta_2,..\\theta_p|$**X**)$d\\theta_2\\theta_3...\\theta_p$\n",
    "\n",
    "Using the sample $p(\\theta_1|$**X**) $\\sim \\theta_{1,1} ... \\theta_{1,B}$\n",
    "\n",
    "*That is, record the parameter of interest $\\theta_1$ from each sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = sampler.chain[:,200::3,0].T\n",
    "\n",
    "plt.hist(chain.flatten(), 100)\n",
    "plt.gca().set_yticks([])\n",
    "plt.xlabel(r\"$\\theta_1 = m$\")\n",
    "plt.ylabel(r\"$p(\\theta_1)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Prediction**: \n",
    "   \n",
    "The exact equation $p(\\tilde{X}|$**X**) = $\\int p(\\tilde{X}|\\theta) p(\\theta|$**X**)$d\\theta$\n",
    "\n",
    "Using the sample $p(\\tilde{X}|$**X**) $\\sim \\tilde{x_1} | \\theta_{1} ... \\tilde{x_B} | \\theta_{B}$\n",
    "\n",
    "*That is, take each sample of $\\theta$ and determine a value for $x$.*\n",
    "\n",
    "We did that before -- making example lines given a selection of $\\theta = m, b$ from the MCMC chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some general advice for checking goodness of fit:\n",
    "\n",
    "Determining whether an MCMC has converged can be difficult, especially in high-dimensional parameter spaces: \"This can be a difficult subject to discuss because it isn’t formally possible to guarantee convergence for any but the simplest models, and therefore any argument that you make will be circular and heuristic.\"\n",
    "\n",
    "Here's a basic overview of how you can convince yourself and others you are on the right track.\n",
    "\n",
    "- **First check** - start multiple chains from different starting values and see that they converge to the same place\n",
    "- **More formal methods** - Raftery-Lewis, Geweke, autocorrelation, etc.\n",
    "- **Goodness of fit**  Posterior Predictive Checks which simulate data from your fitted model and compare to the observed data (checks convergence AND the suitability of the chosen model)\n",
    "\n",
    "See https://pkgw.github.io/mcmc-reporting/ and https://rlhick.people.wm.edu/stories/bayesian_5.html for some details and insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Diagnostics\n",
    "\n",
    "A number of diagnostics (both formal and informal) exist:\n",
    "\n",
    "- Geweke score: compares mean of beginning of chain with mean of end\n",
    "\n",
    "Geweke score = $\\frac{\\bar{\\theta}_e - \\bar{\\theta}_b}{\\sqrt{Var(\\theta_e) + Var(\\theta_b)}}$\n",
    "\n",
    "- Gelman-Rubin: compare variance between chains to variance of single chain.  For a well converged chain the G-R stat should approach 1. Values greater than typically 1.1 indicate that the chains have not yet fully converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see http://joergdietrich.github.io/emcee-convergence.html\n",
    "def gelman_rubin(chain):\n",
    "    ssq = np.var(chain, axis=1, ddof=1)\n",
    "    W = np.mean(ssq, axis=0)\n",
    "    θb = np.mean(chain, axis=1)\n",
    "    θbb = np.mean(θb, axis=0)\n",
    "    m = chain.shape[0]\n",
    "    n = chain.shape[1]\n",
    "    B = n / (m - 1) * np.sum((θbb - θb)**2, axis=0)\n",
    "    var_θ = (n - 1) / n * W + 1 / n * B\n",
    "    R̂ = np.sqrt(var_θ / W)\n",
    "    return R̂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelman_rubin(sampler.chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8.9, 5.5))\n",
    "xmin = 5\n",
    "chain = sampler.chain\n",
    "chain_length = chain.shape[1]\n",
    "step_sampling = np.arange(xmin, chain_length, 50)\n",
    "rhat = np.array([gelman_rubin(chain[:, :steps, :])[0] for steps in step_sampling])\n",
    "plt.plot(step_sampling, rhat, linewidth=2)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.set_xlim(0,1000)\n",
    "xmax = ax.get_xlim()[1]\n",
    "plt.hlines(1.1, xmin, xmax, linestyles=\"--\")\n",
    "plt.ylabel(\"$\\hat R$\")\n",
    "plt.xlabel(\"chain length\")\n",
    "plt.ylim(1, 2)\n",
    "legend = plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that Gelman-Rubin expects chains to be independent and emcee, as designed, imposes correlations on its chains. So proceed with caution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceptance Fraction \n",
    "\n",
    "From https://pkgw.github.io/mcmc-reporting/ : you should report the \"jump acceptance fractions computed for each chain, or a summary of them if there are many chains and/or parameters. Acceptance fractions outside the range of 10–90% suggest that the sampler is not well-matched to your problem and are cause for concern, since your samples may not be fully exploring the posterior distribution.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.acceptance_fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation\n",
    "\n",
    "A good heuristic for assessing convergence of samplings is the integrated autocorrelation time.  The integrated autocorrelation time quantifies \"the effects of sampling error on your results. The basic idea is that the samples in your chain are not independent and you must estimate the effective number of independent samples.\" (https://emcee.readthedocs.io/en/latest/tutorials/autocorr/#autocorr). \n",
    "\n",
    "See also [these lecture notes](https://pdfs.semanticscholar.org/0bfe/9e3db30605fe2d4d26e1a288a5e2997e7225.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emcee.autocorr import integrated_time\n",
    "chain = sampler.chain\n",
    "tau = np.mean([integrated_time(walker, c=2) for walker in chain], axis=0)\n",
    "tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should generally chose a burn in that is at least ~5 $\\times$ these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
